{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Discussion] Pytorch CustomDatset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPg2JDziVxBb3dKTR2Tk87D"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCdBZrVdQBLm",
        "colab_type": "text"
      },
      "source": [
        "해당 repository 및 code들은 wikidocs에서 제공하는 'Pytorch로 시작하는 딥 러닝 입문'을 참고하였음을 밝힙니다. 해당 자료를 바탕으로 숙지한 개념과 추가적인 저의 생각을 기록할 계획입니다. 출처는 다음 사이트와 같습니다.<br><br>\n",
        "* https://wikidocs.net/book/2788"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3D8DN7PQVGS",
        "colab_type": "text"
      },
      "source": [
        "~~~python\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Dataset 상속\n",
        "class CustomDataset(Dataset): \n",
        "  def __init__(self):\n",
        "    self.x_data = [[73, 80, 75],\n",
        "                   [93, 88, 93],\n",
        "                   [89, 91, 90],\n",
        "                   [96, 98, 100],\n",
        "                   [73, 66, 70]]\n",
        "    self.y_data = [[152], [185], [180], [196], [142]]\n",
        "\n",
        "  # 총 데이터의 개수를 리턴\n",
        "  def __len__(self): \n",
        "    return len(self.x_data)\n",
        "\n",
        "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
        "  def __getitem__(self, idx): \n",
        "    x = torch.FloatTensor(self.x_data[idx])\n",
        "    y = torch.FloatTensor(self.y_data[idx])\n",
        "    return x, y\n",
        "\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "model = torch.nn.Linear(3,1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
        "\n",
        "nb_epochs = 20\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    # print(batch_idx)\n",
        "    # print(samples)\n",
        "    x_train, y_train = samples\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "    # cost로 H(x) 계산\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
        "        cost.item()\n",
        "        ))\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxFyiSwNQZp5",
        "colab_type": "text"
      },
      "source": [
        "Keras로 구현된 github repository나 Kaggle을 보면서 custom dataset을 바탕으로 transfer learning을 적용할 때 CustomDataset이라는 클래스를 선언한 코드들을 종종 보곤 했다. 처음에는 관행이라고 생각했는데, (물론 진짜 관행일수도 있지만) 파이토치를 공부하면서 그 이유를 짐작할 수 있었다. 파이토치의 클래스 기반 구현 방식을 차용해왔다고 생각한다. 위의 코드 또한 CustomDataset이라는 클래스를 선언하여 custom dataset을 관리한다.<br>\n",
        "내가 사용해야 할 모델은 실력 좋으신 선배님들이 논문을 통해서 (지금 이 순간조차도) 꾸준히 연구하며 발표하고 있다. 현재로써 나의 역할은 발표된 모델을 내가 해결하고자 하는 문제에 적용할 수 있는 능력을 기르는 것인데, 그것을 위한 첫 번째 능력이 custom datset을 모델에 적합하게 전처리하는 능력이라고 생각한다. 따라서 해당 파트를 제대로 이해하고 응용하는 연습을 해야 할 것이다."
      ]
    }
  ]
}